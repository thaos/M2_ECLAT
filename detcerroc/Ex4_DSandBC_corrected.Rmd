---
title: "Exercice 4 : descente d'échelle statistique et correction de bias"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. Execute a chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

## Objectifs
Le but est de presenter le principe de fonctionnement de la descente d'échelle statistique et de la correction. Pour cela, nous appliquerons quelques méthodes de base. Deux sont des méthodes dites de Perfect Prognosis et les deux autres de la catégorie Model Output Statstics.

## Downscalling

### Lecture des données
```{r}
tas_df <- readRDS("Ex1_tas_df.rds")

# Downscaling
X <- tas_df[, "ERA5"]
Y <- tas_df[, "GHCND_orly"]
ina <- is.na(X) | is.na(Y)
X <- X[!ina]
Y <- Y[!ina]
dates <- as.numeric(rownames(tas_df))
dates <- dates[!ina]
```

### Séparation en base d'apprentissage et base de test (calibration/validation)

```{r}
iapp <- dates < 20000101
X_app <- X[iapp]
Y_app <- Y[iapp]
X_test <- X[!iapp]
Y_test <- Y[!iapp]
```


### Comparaison entre X et Y
```{r, fig.width = 6, fig.height = 9}
# Root Mean Square Error (RMSE)
sqrt(mean((Y_app - X_app)^2))
# Correlation
cor(X_app, Y_app)
sqrt(mean((Y_test - X_test)^2))
cor(X_test, Y_test)

# Scatterplots
par(mfrow = c(2, 1), pty = "s")
xylim <- range(X, Y, na.rm = TRUE)
plot(X_app, Y_app, xlim = xylim, ylim = xylim)
abline(b = 1, a = 0, col = "red")
plot(X_test, Y_test, xlim = xylim, ylim = xylim)
abline(b = 1, a = 0, col = "red")

# Différences des séries temporelles
par(mfrow = c(2, 1), pty = "m")
ylim = range(Y_app - X_app, Y_test - X_test)
plot(Y_app - X_app, type = "l", ylim = ylim)
plot(Y_test - X_test, type = "l", ylim = ylim)

# quantile-quantile plot (QQ-plot)
par(mfrow = c(2, 1), pty = "s")
qqplot(X_app, Y_app, xlim = xylim, ylim = xylim, main = "QQ-plot")
abline(b = 1, a = 0, col = "red")
qqplot(X_test, Y_test, xlim = xylim, ylim = xylim, main = "QQ-plot")
abline(b = 1, a = 0, col = "red")
```

### Perfect Prog
#### Linear Model
Utiliser la fonction lm pour ajuster un modéle linéaire pour expliquer Y en fonction de X
Utiliser la fonction predict pour appliquer le modèle appris à la base d'apprentissage et de test.
Comparer les prédictions aux vrais valeurs de Y.
Le modèle appris est il aussi sur la base de test que sur la base d'apprentissage ?
```{r, fig.width = 6, fig.height = 9}
base_app <- data.frame(X = X_app, Y = Y_app)
base_test <- data.frame(X = X_test, Y = Y_test)
lm_fit <- lm(Y ~ X, data = base_app) # à compléter
summary(lm_fit)

lm_app <- predict(lm_fit)  # à compléter
lm_test<- predict(lm_fit, newdata = base_test)  # à compléter

# RMSE et correlation entre valeurs prédites et réelles de Y sur base d'apprentissage
sqrt(mean((Y_app - lm_app)^2)) # à compléter
cor(lm_app, Y_app) # à compléter
# RMSE et correlation entre valeurs prédites et réelles de Y sur base de test
sqrt(mean((Y_test - lm_test)^2)) # à compléter
cor(lm_test, Y_test) # à compléter


# diagnostics graphiques sur bases d'apprentissage et de test
par(mfrow = c(2, 1), pty = "s")
plot(lm_app, Y_app, xlim = xylim, ylim = xylim)
abline(b = 1, a = 0, col = "red")
plot(lm_test, Y_test, xlim = xylim, ylim = xylim)
abline(b = 1, a = 0, col = "red")

ylim = range(Y_app - lm_app, Y_test - lm_test)
par(mfrow = c(2, 1), pty = "m")
plot(Y_app - lm_app, type = "l", ylim = ylim)
plot(Y_test - lm_test, type = "l", ylim = ylim)

par(mfrow = c(2, 1), pty = "s")
qqplot(lm_app, Y_app, xlim = xylim, ylim = xylim, main = "QQ-plot")
abline(b = 1, a = 0, col = "red")
qqplot(lm_test, Y_test, xlim = xylim, ylim = xylim, main = "QQ-plot")
abline(b = 1, a = 0, col = "red")
```

#### analogues / nearest-neighbour
Appliquer la descente d'échelle par analogue.
Pour cela, il faut calculer la distance entre les valeurs de X de la base d'apprentissage et les valeurs de X pour les quelles on souhaite prédire la valeur en Y.
fonction utile: which.min
Réaliser les mêmes dignostics que pour le modèle linéaire.
```{r, fig.width = 6, fig.height = 9}
nn_app <- numeric(length(Y_app))
for( i in seq_along(nn_app)){
  dist <- (X_app - X_app[i])^2
  inn <- which.min(dist)
  nn_app[i] <- Y_app[inn]
}

nn_test <- numeric(length(Y_test))
for( i in seq_along(nn_test)){
  dist <- (X_app - X_test[i])^2
  inn <- which.min(dist)
  nn_test[i] <- Y_app[inn]
}

sqrt(mean((Y_app - nn_app)^2))
cor(nn_app, Y_app)
sqrt(mean((Y_test - nn_test)^2))
cor(nn_test, Y_test)

par(mfrow = c(2, 1), pty = "s")
plot(nn_app, Y_app, xlim = xylim, ylim = xylim)
abline(b = 1, a = 0, col = "red")
plot(nn_test, Y_test, xlim = xylim, ylim = xylim)
abline(b = 1, a = 0, col = "red")

ylim = range(Y_app - nn_app, Y_test - nn_test)
par(mfrow = c(2, 1), pty = "m")
plot(Y_app - nn_app, type = "l", ylim = ylim)
plot(Y_test - nn_test, type = "l", ylim = ylim)

par(mfrow = c(2, 1), pty = "s")
qqplot(nn_app, Y_app, xlim = xylim, ylim = xylim, main = "QQ-plot")
abline(b = 1, a = 0, col = "red")
qqplot(nn_test, Y_test, xlim = xylim, ylim = xylim, main = "QQ-plot")
abline(b = 1, a = 0, col = "red")
```

### Model Output Statistic
#### simple mean correction

Appliquer la descente d'échelle par correction de la moyenne.
Réaliser les mêmes dignostics que pour le modèles linéaires.

```{r, fig.width = 6, fig.height = 9}
mc_app <- X_app + mean(Y_app) - mean(X_app)
mc_test <- X_test + mean(Y_app) - mean(X_app)

sqrt(mean((Y_app - mc_app)^2))
cor(mc_app, Y_app)
sqrt(mean((Y_test - mc_test)^2))
cor(mc_test, Y_test)

par(mfrow = c(2, 1), pty = "s")
plot(mc_app, Y_app, xlim = xylim, ylim = xylim)
abline(b = 1, a = 0, col = "red")
plot(mc_test, Y_test, xlim = xylim, ylim = xylim)
abline(b = 1, a = 0, col = "red")

ylim = range(Y_app - mc_app, Y_test - mc_test)
par(mfrow = c(2, 1), pty = "m")
plot(Y_app - mc_app, type = "l", ylim = ylim)
plot(Y_test - mc_test, type = "l", ylim = ylim)

par(mfrow = c(2, 1), pty = "s")
qqplot(mc_app, Y_app, xlim = xylim, ylim = xylim, main = "QQ-plot")
abline(b = 1, a = 0, col = "red")
qqplot(mc_test, Y_test, xlim = xylim, ylim = xylim, main = "QQ-plot")
abline(b = 1, a = 0, col = "red")
```

#### quantile-quantile correction
Appliquer la descente d'échelle via la correction quantile quantile.
Fonctions utiles: ecdf, quantile
Réaliser les mêmes dignostics que pour le modèles linéaires.

```{r, fig.width = 6, fig.height = 9}
qq_app <- quantile(Y_app, probs = ecdf(X_app)(X_app))
qq_test <- quantile(Y_app, probs = ecdf(X_app)(X_test))

sqrt(mean((Y_app - qq_app)^2))
cor(qq_app, Y_app)
sqrt(mean((Y_test - qq_test)^2))
cor(qq_test, Y_test)

par(mfrow = c(2, 1), pty = "s")
plot(qq_app, Y_app, xlim = xylim, ylim = xylim)
abline(b = 1, a = 0, col = "red")
plot(qq_test, Y_test, xlim = xylim, ylim = xylim)
abline(b = 1, a = 0, col = "red")

ylim = range(Y_app - qq_app, Y_test - qq_test)
par(mfrow = c(2, 1), pty = "m")
plot(Y_app - qq_app, type = "l", ylim = ylim)
plot(Y_test - qq_test, type = "l", ylim = ylim)

par(mfrow = c(2, 1), pty = "s")
qqplot(qq_app, Y_app, xlim = xylim, ylim = xylim, main = "QQ-plot")
abline(b = 1, a = 0, col = "red")
qqplot(qq_test, Y_test, xlim = xylim, ylim = xylim, main = "QQ-plot")
abline(b = 1, a = 0, col = "red")
```

### Summary plots
#### Apprentissage
Quelle est la méthode le plus performante sur la base d'apprentissage ?
```{r, fig.width = 6, fig.height = 9}
# Point by point
par(pty = "s")
plot(X_app, Y_app, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5)
points(lm_app, Y_app, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 2)
points(nn_app, Y_app, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 3)
points(mc_app, Y_app, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 4)
points(qq_app, Y_app, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 5)
abline(b = 1, a = 0, col = "red")
legend(
  "topleft",
  legend = c("X", "lm", "nn", "mc", "qq"),
  col = 1:5,
  pch = 20
)
par(pty = "s") 
ylim <- range(c(X_app, lm_app, nn_app, mc_app, qq_app) -  Y_app)
plot(X_app - Y_app, ylim = ylim, type = "l")
lines(lm_app - Y_app, ylim = ylim, col = 2)
lines(nn_app - Y_app, ylim = ylim, col = 3)
lines(mc_app - Y_app, ylim = ylim, col = 4)
lines(qq_app - Y_app, ylim = ylim, col = 5)
abline(h = 0, col = "red")
legend(
  "topleft",
  legend = c("X", "lm", "nn", "mc", "qq"),
  col = 1:5,
  pch = 20
)
# distribution
qqplot_lm <- qqplot(lm_app, Y_app, plot.it = FALSE)
qqplot_nn <- qqplot(nn_app, Y_app, plot.it = FALSE)
qqplot_mc <- qqplot(mc_app, Y_app, plot.it = FALSE)
qqplot_qq <- qqplot(qq_app, Y_app, plot.it = FALSE)
par(pty = "s")
qqplot(X_app, Y_app, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, main = "QQ-plot")
points(qqplot_lm$x, qqplot_lm$y, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 2)
points(qqplot_nn$x, qqplot_nn$y,, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 3)
points(qqplot_mc$x, qqplot_mc$y,, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 4)
points(qqplot_qq$x, qqplot_qq$y,, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 5)
abline(b = 1, a = 0, col = "red")
legend(
  "topleft",
  legend = c("X", "lm", "nn", "mc", "qq"),
  col = 1:5,
  pch = 20
)

# test d'égalité des distribution (à développer si il y a le temps).
ks.test(X_app, Y_app)
ks.test(lm_app, Y_app)
ks.test(nn_app, Y_app)
ks.test(mc_app, Y_app)
ks.test(qq_app, Y_app)
```

#### Test
Quelle est la méthode le plus performante sur la base de test ?

```{r, fig.width = 6, fig.height = 9}
# Point by point
par(pty = "s")
plot(X_test, Y_test, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5)
points(lm_test, Y_test, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 2)
points(nn_test, Y_test, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 3)
points(mc_test, Y_test, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 4)
points(qq_test, Y_test, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 5)
abline(b = 1, a = 0, col = "red")
legend(
  "topleft",
  legend = c("X", "lm", "nn", "mc", "qq"),
  col = 1:5,
  pch = 20
)

# distribution
qqplot_lm <- qqplot(lm_test, Y_test, plot.it = FALSE)
qqplot_nn <- qqplot(nn_test, Y_test, plot.it = FALSE)
qqplot_mc <- qqplot(mc_test, Y_test, plot.it = FALSE)
qqplot_qq <- qqplot(qq_test, Y_test, plot.it = FALSE)
par(pty = "s")
qqplot(X_test, Y_test, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, main = "QQ-plot")
points(qqplot_lm$x, qqplot_lm$y, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 2)
points(qqplot_nn$x, qqplot_nn$y,, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 3)
points(qqplot_mc$x, qqplot_mc$y,, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 4)
points(qqplot_qq$x, qqplot_qq$y,, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 5)
abline(b = 1, a = 0, col = "red")
legend(
  "topleft",
  legend = c("X", "lm", "nn", "mc", "qq"),
  col = 1:5,
  pch = 20
)
ks.test(X_test, Y_test)
ks.test(lm_test, Y_test)
ks.test(nn_test, Y_test)
ks.test(mc_test, Y_test)
ks.test(qq_test, Y_test)
```

## Bias Corretion: only MOS
#### Lecture des données
```{r}
X <- tas_df[, "IPSL-CM5A-LR_historical_r1i1p1"]
Y <- tas_df[, "GHCND_orly"]
ina <- is.na(X) | is.na(Y)
X <- X[!ina]
Y <- Y[!ina]
dates <- as.numeric(rownames(tas_df))
dates <- dates[!ina]
```


### Séparation en base d'apprentissage et base de test (calibration/validation)
```{r}
iapp <- dates < 20000101
X_app <- X[iapp]
Y_app <- Y[iapp]
X_test <- X[!iapp]
Y_test <- Y[!iapp]
```

### Comparaison entre X et Y
```{r, fig.width = 6, fig.height = 9}
# A ne pas mettre
sqrt(mean((Y_app - X_app)^2))
cor(X_app, Y_app)
sqrt(mean((Y_test - X_test)^2))
cor(X_test, Y_test)

xylim <- range(X, Y, na.rm = TRUE)

par(mfrow = c(2, 1), pty = "s")
plot(X_app, Y_app, xlim = xylim, ylim = xylim)
abline(b = 1, a = 0, col = "red")
plot(X_test, Y_test, xlim = xylim, ylim = xylim)
abline(b = 1, a = 0, col = "red")

ylim = range(Y_app - X_app, Y_test - X_test)
par(mfrow = c(2, 1), pty = "m")
plot(Y_app - X_app, type = "l", ylim = ylim)
plot(Y_test - X_test, type = "l", ylim = ylim)

ylim = range(Y_app - qq_app, Y_test - qq_test)
qqplot(X_app, Y_app, xlim = xylim, ylim = xylim, main = "QQ-plot")
abline(b = 1, a = 0, col = "red")
qqplot(X_test, Y_test, xlim = xylim, ylim = xylim, main = "QQ-plot")
abline(b = 1, a = 0, col = "red")
```

### Model Output Statistic
#### simple mean correction
Réaliser une correction de bias via la méthode de correction de la moyenne.

```{r, fig.width = 6, fig.height = 9}
# A ne pas mettre
mc_app <- X_app + mean(Y_app) - mean(X_app)
mc_test <- X_test + mean(Y_app) - mean(X_app)

par(mfrow = c(2, 1), pty = "s")
qqplot(mc_app, Y_app, xlim = xylim, ylim = xylim, main = "QQ-plot")
abline(b = 1, a = 0, col = "red")
qqplot(mc_test, Y_test, xlim = xylim, ylim = xylim, main = "QQ-plot")
abline(b = 1, a = 0, col = "red")
```

#### quantile-quantile correction
Réaliser une correction de bias via la méthode quantile quantile.
```{r, fig.width = 6, fig.height = 9}
# A ne pas mettre
qq_app <- quantile(Y_app, probs = ecdf(X_app)(X_app))
qq_test <- quantile(Y_app, probs = ecdf(X_app)(X_test))

par(mfrow = c(2, 1), pty = "s")
qqplot(qq_app, Y_app, xlim = xylim, ylim = xylim, main = "QQ-plot")
abline(b = 1, a = 0, col = "red")
qqplot(qq_test, Y_test, xlim = xylim, ylim = xylim, main = "QQ-plot")
abline(b = 1, a = 0, col = "red")
```

### Summary plots
#### Apprentissage
Quelle est la méthode le plus performante sur la base d'apprentissage ?

```{r, fig.width = 6, fig.height = 9}
# A ne pas mettre
# distribution
qqplot_mc <- qqplot(mc_app, Y_app, plot.it = FALSE)
qqplot_qq <- qqplot(qq_app, Y_app, plot.it = FALSE)

par(pty = "s")
qqplot(X_app, Y_app, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, main = "QQ-plot")
points(qqplot_mc$x, qqplot_mc$y,, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 4)
points(qqplot_qq$x, qqplot_qq$y,, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 5)
abline(b = 1, a = 0, col = "red")
legend(
  "topleft",
  legend = c("X", "mc", "qq"),
  col = c(1, 4, 5),
  pch = 20
)
ks.test(X_app, Y_app)
ks.test(mc_app, Y_app)
ks.test(qq_app, Y_app)
```

#### Test
Quelle est la méthode le plus performante sur la base de test ?

```{r, fig.width = 6, fig.height = 9}
# A ne pas mettre
# distribution
qqplot_mc <- qqplot(mc_test, Y_test, plot.it = FALSE)
qqplot_qq <- qqplot(qq_test, Y_test, plot.it = FALSE)
par(pty = "s")
qqplot(X_test, Y_test, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, main = "QQ-plot")
points(qqplot_mc$x, qqplot_mc$y,, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 4)
points(qqplot_qq$x, qqplot_qq$y,, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 5)
abline(b = 1, a = 0, col = "red")
legend(
  "topleft",
  legend = c("X", "mc", "qq"),
  col = 1:5,
  pch = 20
)
ks.test(X_test, Y_test)
ks.test(mc_test, Y_test)
ks.test(qq_test, Y_test)
```


## Perfect Prognosis: Appliquer le downscaling aux simulations climatiques.

Est-ce que c'est raisonnable ?

```{r, fig.width = 6, fig.height = 9}
base_gcm <- data.frame(X = X_test, Y = Y_test)
lm_gcm <- predict(lm_fit, newdata = base_gcm)
nn_gcm <- numeric(length(Y_test))
for(i in seq_along(nn_test)){
  dist <- (base_app$X - X_test[i])^2
  inn <- which.min(dist)
  nn_gcm[i] <- base_app$Y[inn]
}
```

### Summary plots
#### base de test
```{r, fig.width = 6, fig.height = 9}
# A ne pas mettre
# distribution
qqplot_lm <- qqplot(lm_gcm, Y_test, plot.it = FALSE)
qqplot_nn <- qqplot(nn_gcm, Y_test, plot.it = FALSE)
par(pty = "s")
qqplot(X_test, Y_test, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, main = "QQ-plot")
points(qqplot_mc$x, qqplot_mc$y,, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 4)
points(qqplot_qq$x, qqplot_qq$y,, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 5)
points(qqplot_lm$x, qqplot_lm$y,, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 2)
points(qqplot_nn$x, qqplot_nn$y,, xlim = xylim, ylim = xylim, pch = 20, cex = 0.5, col = 3)
abline(b = 1, a = 0, col = "red")
legend(
  "topleft",
  legend = c("X", "lm", "nn", "mc", "qq"),
  col = 1:5,
  pch = 20
)
ks.test(X_test, Y_test)
ks.test(lm_test, Y_test)
ks.test(nn_test, Y_test)
``` 
## Quelles améliorations pouvez-vous proposer ?