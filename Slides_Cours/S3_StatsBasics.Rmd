---
title: "Séance 3"
subtitle: "Bases de statistique l'étude des aléas climatiques"
author: "Soulivanh Thao <br/> sthao@lsce.ipsl.fr"
institute: "LSCE, ESTIMR"
date: "2020/10/01 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


# Séquence 2: Données et outils statistiques

- Séance 2: Pourquoi les statistiques pour étudier le climat? Avec quelles données ? + Séance pratique en R.

- Séance 3: Introduction aux notions statistiques de façon plus formelle

- Séance 4: Séances pratique en R sur les effets de l'aggrégation spatiale et l'effet de la variabilité interne sur les estimations statistiques.

- Séance 5: Introduction au downscaling statistique et correction de biais par la pratique.

Il y a beaucoup de pratique avec le logiciel R. Il vous faut donc l'installer ainsi que Rstudio.
Vous pouvez faire [ces exercices](https://github.com/thaos/MathDACC_tutoR/blob/master/Rtuto_basics/basics_tuto_exercices.R) pour vous familiariser avec R. La correction est disponible [ici](https://github.com/thaos/MathDACC_tutoR/blob/master/Rtuto_basics/basics_tuto.R.completed).

---
# Problèmes typiques

- Quelle température annuelle moyenne seront nous susceptible d'observer en 2051 ?
- Quelle est la probabilité que la température dépasse 290 K en 1950 ? en 2050 ?
- Quelle est la probabilité que la température dépasse 320 K en 1950 ? en 2050 ?
- Quelle seuil de température peut ont espérer dépasser au moins une fois tous les 50 ans ?

```{r, echo=FALSE, fig.height=4, dev='svg'}
tas_stagg <- readRDS(file = "../Ex2_aggregation/tas_stagg.rds")
tas <- readRDS(file = "../Ex2_aggregation/tas.rds")
dates <- dimnames(tas)[[3]]
years <- unique(as.integer(substring(dates, 1, 4)))

tas_smean_tmean <- tas_stagg[[1]][[1]]  

zones <- dimnames(tas_smean_tmean)[[2]]
periods <- dimnames(tas_smean_tmean)[[3]]
lon <- as.numeric(rownames(tas))
lat <- as.numeric(colnames(tas))

plot(
  years, tas_smean_tmean[, 1, 1], #* 3600 * 24 * 365,
  xlab = "year", ylab = "tas (K)", main = "tas, annual mean",
  pch = 20
  )
```

---
# Rappels sur les variables aléatoires continues


La distribution d'une variable aléatoire univariée et continue $X$ est entièrement déterminée par sa fonction de répartition (cumulative distribution function) 

$$F(x) = Pr (X \le x) $$
    
**Remarque** : on dit que $x_\alpha$ est le quantile de niveau $\alpha$ si $F(x_\alpha) = \alpha$
    
Si $F$ est une fonction continue, alors il existe une fonction positive $f$, appelé la densité de probabilité (probability density function) tel que

$$ Pr(a\le X\le b) = F(b) - F(a) = \int_a^b f(x)\,dx$$.


---
# cdf and pdf

```{r cdfpdf, echo=FALSE, fig.height=4, dev='svg'}
par(mfrow = c(1, 2))
x <- seq(-4, 4, by = 0.01)
plot(
  x, pnorm(x),
  main = "cdf", xlab = "x", ylab = "F(x)",
  type = "l", ylim = c(-0.1, 1)
)
abline(h = c(0, 1), lty = 2, col = "grey")
xstar <- -0.5
# xpol <- c(x[x < xstar], rep(xstar, 2))
# ypol <- c(pnorm(x[x < xstar]), pnorm(xstar), 0)
# polygon(xpol, ypol, col = "pink", border = NA)
# lines(x, pnorm(x))
segments(
  x0 = c(xstar, xstar),
  y0 = c(-1, pnorm(xstar)),
  x = c(xstar, -5),
  y = c(pnorm(xstar), pnorm(xstar)),
  lty = 2, col = "red"
)
text(
  x = c(-3.5, 0),
  y = c(0.35, -0.07),
  labels = c("F(a)", "a"),
  col = "red"
)

plot(
  x, dnorm(x),
  main = "pdf", xlab = "x", ylab = "f(x)",
  type = "l", ylim = c(-0.02, max(dnorm(x)))
)
abline(h = 0, lty = 2, col = "grey")
xstar <- -0.5
xpol <- c(-5, x[x < xstar], rep(xstar, 2))
ypol <- c(0, dnorm(x[x < xstar]), dnorm(xstar), 0)
polygon(xpol, ypol, col = "pink", border = NA)
lines(x, dnorm(x))
segments(
  x0 = xstar,
  y0 = -1,
  x = xstar,
  y = dnorm(xstar),
  lty = 2, col = "red"
)
text(
  x = c(-1.3, 0),
  y = c(0.02, -0.015),
  labels = c("F(a)", "a"),
  col = "red"
)
```

---
# Esperance

=== Variable continue à densité ===
Si la variable aléatoire continue réelle $X$ admet une densité de probabilité $f$, son espérance est définie comme :
$$\mathbb{E}[X] = \int_{-\infty}^\infty x f(x)\, \mathrm{d}x$$
à condition que l'intégrale soit absolument convergente.

---
# Esperance (quelques propriétés)

**Linéarité**:
l'espérance est un opérateur linéaire. Pour deux variables aléatoires quelconques $X$ et $Y$  et pour deux nombres réels $a$ et $b$:
$$\mathbb E[aX+bY]=a\mathbb E(X)+b\mathbb E(Y)$$


**espérance d'une fonction d'une variable aléatoire**:
Si $X$ est une variable aléatoire absolument continue réelle, de densité de probabilité $f$, alors l'espérance d'une fonction mesurable de $X$, $g(X)$ est:

$$\mathbb E \left[ g(X) \right] =  \int_{-\infty}^\infty g(x) f(x) \mathrm{d}x$$




---
# Estimateur empirique de l'esperance 

**Théorème (Khintchine) : loi faible des grands nombres**

Soit $X_n$ une suite de variables aléatoires réelles indépendantes et identiquement distribuées(iid) admettant une espérance $\mu = \mathbb{E}[X]$.

La moyenne empirique 
$$\overline{X}_n = \frac{1}{n}\sum_{k=1}^n X_k$$
converge en probabilité vers cette espérance : pour tout $\varepsilon > 0$, on a 
$$\lim_{n\to+\infty} \mathbb{P}\left(\left|\overline{X}_n-\mu\right| > \varepsilon\right) = 0$$ 

Ce résultat assure en particulier que la moyenne empirique est un estimateur convergent de l'espérance.

**Question** Calculer l'espérance de $\overline{X}_n$
---
# Variance


Dans le cas d’une variable aléatoire à densité, la variance est définie par :
$$ \operatorname{Var}(X) = \sigma^2 =  \int_{-\infty}^\infty  (x-\mu)^2  f(x) \mathrm{d}x $$

où $f$ est la densité de probabilité de $X$ et $\mu$ son l'espérance mathématique $X$ 


La variance d'une variable aléatoire continue {{mvar|X}} peut aussi se calculer de la façon suivante :
$$  \operatorname{Var}(X) = \sigma^2 = \mathbb{E}[X^2] - \mu^2$$
Si $\sigma^2$ est la variance de $X$, $\sigma$ correspond à son écart-type.


---
# Variance (quelques propriétés)

**Transformation affine**
$$ \operatorname{Var}(aX+b) =  a^2\operatorname{Var}(X)$$
**Combinaison linéaire**
Si deux variables aléatoires $X$ et $Y$ admettent une variance, alors 
$$\operatorname{Var}(X + Y) = \operatorname{Var}(X) + \operatorname{Var}(Y) + 2 \operatorname{Cov}(X, Y)$$
où $\operatorname{Cov}(X, Y)$ est la covariance

---
# Estimateur empirique de la variance
Soit $X_n$ une suite de variables aléatoires réelles indépendantes et identiquement distribuées(iid),  la variance $\sigma^2$ de cette loi peut être estimée à l’aide de la variance empirique

$$S^2 = \frac{1}{n}\sum_{i=1}^n(X_i-\overline{X})^2$$
où $\overline{X} = \frac{1}{n}\sum_{i=1}^n X_i$ est la [[moyenne empirique]].

Cet estimateur est cependant biaisé, car $\mathbb{E}(S^2) = \frac{n-1}{n}\sigma^2$.


---
# Covariance

La covariance de deux variables aléatoires réelles $X$ et $Y$ ayant chacune une variance finie est définie par
$$ \operatorname{Cov}(X,Y) = \mathbb{E}[(X-\mathbb{E}[X])\,(Y-\mathbb{E}[Y])$$
De manière équivalente :
$$ \operatorname{Cov}(X,Y) = \operatorname{E}(XY) - \operatorname{E}(X) \operatorname{E}(Y)$$


**Remarque :**  La variance de $X$ est donc $\operatorname{Var}(X) = \operatorname{Cov}(X, X)$.

**Question :** Comment estimer  $\operatorname{Cov}(X,Y)$ ?
---
# Correlation (de Pearson)

Le coefficient de corrélation entre deux variables aléatoires réelles $X$ et $Y$ ayant chacune une variance finie, noté $\operatorname{Cor}(X,Y)$, ou parfois $\rho_{XY}$, ou simplement $r$,  est défini par :
$$ r = \frac{\operatorname{Cov}(X,Y)}{\sigma_X \sigma_Y}$$
où $\sigma_X$ et $\sigma_Y$ désignent respectivement les écarts types de $X$ et $Y$.

**Question: ** Comment estimer  $\operatorname{Cor}(X,Y)$ ?


---
# Independance et correlation
La valeur du coefficient de corrélation est comprise entre  −1 et  +1.

Elle indique le degré de dépendance linéaire entre deux variable :

- 1 correspond à une parfaite relation linéaire croissante

- -1 correspond à une parfaite relation inverse décroissante. 

- 0 indique une absence de relation linéaire entre les variables.


**Attention !**

$$ 
\begin{align}
X,Y \text{ independent} \quad & \Rightarrow \quad \rho_{X,Y} = 0 \quad (X,Y \text{ uncorrelated})\\
\rho_{X,Y} = 0 \quad (X,Y \text{ uncorrelated})\quad & \nRightarrow \quad X,Y \text{ independent}
\end{align}
$$
---
# Exemples de corrélation
![](Figs/Correlation_examples2.png)
---
# Loi jointe de 2 variables aléatoires continues

###Fonction de répartition bivariée
$$F_{XY}(x,y) = \mathbb{P}(X\leq x,Y\leq y)$$

<p align="center">
<img src="Figs/multivariate_gaussian.svg"/>
<br/>
<a href="http://motion.pratt.duke.edu/RoboticSystems/Probability.html">Source Kris Hauser</a>
</p>

**Remarque : ** $F_{XY}(x, +\infty) = F_{X}(x)$



---
#Loi jointe de 2 variables aléatoires continues

###Densité de probabilité bivariée
$$f_{XY}(x,y) = \frac{\partial^2}{\partial x\,\partial y}F_{XY}(x,y)$$
<p align="center">
<img src="Figs/multivariate_gaussian.svg"/>
<br/>
<a href="http://motion.pratt.duke.edu/RoboticSystems/Probability.html">Source Kris Hauser</a>
</p>

**Remarques :** 
- $f_X (x) = \int_{-\infty}^\infty f_{XY}(x,y)\,\mathrm dy$
- $f_{X|Y}(x, y) = \frac {f_{XY}(x,y)} {f_{Y}(y)}$

---
# Dépendence et indépendance

**Pour deux événements**

Deux événements $A$ et $B$ sont indépendants (parfois noté $A \perp B$ or $A \perp\!\!\!\perp B$) si et seulement si 
$$\mathrm{P}(A \cap B) = \mathrm{P}(A)\mathrm{P}(B)$$
}

---
# Dépendence et indépendance

**Deux variable aléatoires réelles**
$X$ and $Y$ sont indépendantes si et seulement si leur fonction de répartition jointe  $F_{X,Y}(x,y) = P(X \leq x, Y \leq y)$ peut s'écrire comme 
$$ F_{X,Y}(x,y) = F_X(x) F_Y(y) \quad \text{for all } x,y.$$



De manière équivalente, si les densités de probabilité  $f_X(x)$ et  $f_Y(y)$ ainsi que la densité de probabilité jointe $f_{X,Y}(x,y)$ existent,

$$ f_{X,Y}(x,y) = f_X(x) f_Y(y) \quad \text{for all } x,y.$$
---
# Indépendance et probabilité conditionnelle 

Si $\mathbb{P}(B) \neq 0$,  la probabilité conditionnelle de l'événement $A$ sachant l'événement $B$, notée $\mathbb{P}(A\mid B)$ est définie par la relation ci-dessous :

$$ \mathbb{P}(A\mid B)={\mathbb{P}(A \cap B) \over \mathbb{P}(B)} = \dfrac{P(B \mid A)P(A)}{P(B)}$$
C'est le théorème de Bayes!

Dans le cas où les évenements sont indépendants, 
$$\mathrm{P}(A \cap B) = \mathrm{P}(A)\mathrm{P}(B) \iff \mathrm{P}(A\mid B) = \frac{\mathrm{P}(A \cap B)}{\mathrm{P}(B)} = \mathrm{P}(A).$$

La probabilité $\mathrm{P}(A\mid B)$ ne dépend donc pas de $B$.


---
# Vraisemblance

Soit $X$ une variable aléatoire (potentiellement multivarié) ayant pour densité de probabilité $f$ dépendant d'un paramètre $\theta$. 

La vraisemblance $\mathcal L$ est une fonction de $\theta$, étant donnée une réalisation $x$ de la variable aléatoire $X$:

$$\mathcal L(\theta | x) = f(x; \theta)$$
---
# Vraisemblance

Soit $X_1, ..., X_n$ des variables aléatoires i.i.d. de loi exponentielle

$$f_{X_1}(x; \theta) = \left\{\begin{matrix}
\theta e^{-\theta x} & \text{si} \; x \geqslant 0 \\
0 & \text{si} \; x < 0
\end{matrix}\right.$$

On observe $x_1, ..., x_n$ les réalisations respectives de $X_1, ..., X_n$

La vraisemblance de $x_1, ..., x_n$ s'écrit

$$\begin{align}
  \mathcal L(\theta | x_1, ..., x_n) &= f_{X_1}(x_1; \theta) \times \ldots \times f_{X_n}(x_n; \theta) \\
  &= \theta^n e^{-\theta (x_1 + \ldots + x_n)}
\end{align}$$

---
# Estimation par maximum de vraisemblance

On cherche $\theta^*$ qui maximise la vraisemblance des données observées $\textbf x = (x_1, \ldots, x_n)$:

$$\theta^* = \underset{\theta}{\mathrm{argmax}} \, \mathcal L(\theta |\textbf x)$$

**Remarque**: maximiser  $\mathcal L(\theta |\textbf x)$ revient à maximiser  $\mathcal{l}(\theta | \textbf{x}) = \log  L(\theta |\textbf{x})$

---
# Exemple avec lois exponentielles i.i.d.

$$\begin{align} 
\underset{\theta}{\mathrm{argmax}} \, \mathcal L(\theta |\textbf x) &= \underset{\theta}{\mathrm{argmax}} \, \theta^n e^{-\theta (x_1 + \ldots + x_n)} \\
&= \underset{\theta}{\mathrm{argmax}} \, \mathcal{l}(\theta |\textbf x) \\
&= \underset{\theta}{\mathrm{argmax}} \log(\exp(n \log(\theta) -\theta (x_1 + \ldots + x_n)) \\
&= \underset{\theta}{\mathrm{argmax}} \, n \log(\theta) -\theta (x_1 + \ldots + x_n)
\end{align}$$

$$\begin{align} 
\frac{\mathrm d \mathcal l(\theta |\textbf x)}{\mathrm d \theta} = 0 &\implies \frac{n}{\theta} - (x_1 + \ldots + x_n) = 0 \\
&\implies \theta = \frac{n}{(x_1 + \ldots + x_n)}
\end{align}$$
---

# Estimateurs du maximum de vraisemblance

### Propriétés


L'estimateur obtenu par la méthode du maximum de vraisemblance est :

* convergent,
* asymptotiquement efficace, il atteint la borne de Cramér-Rao,
* asymptotiquement distribué selon une loi normale.

En revanche, il peut être biaisé en échantillon fini.
---
# Empirical estimator of the cdf

Soit $(X_1, ..., X_n)$ des variables aléatoires réelles, indépendantes et identiquement distribuées (i.i.d) ayant pour cdf $F$. On définit la fonction de répartition empirique (empirical cumulative distribution function) par 

$$\widehat F_n(x) =  \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{X_i \le x}$$

```{r ecdf, echo=FALSE, fig.height=4, dev='svg'}
par(mfrow = c(1, 2))
x <- seq(-4, 4, by = 0.01)
plot(
  x, pnorm(x),
  main = "cdf", xlab = "x", ylab = "F(x)",
  type = "l", ylim = c(-0.1, 1)
)
abline(h = c(0, 1), lty = 2, col = "grey")
xstar <- -0.5
# xpol <- c(x[x < xstar], rep(xstar, 2))
# ypol <- c(pnorm(x[x < xstar]), pnorm(xstar), 0)
# polygon(xpol, ypol, col = "pink", border = NA)
# lines(x, pnorm(x))
segments(
  x0 = c(xstar, xstar),
  y0 = c(-1, pnorm(xstar)),
  x = c(xstar, -5),
  y = c(pnorm(xstar), pnorm(xstar)),
  lty = 2, col = "red"
)
text(
  x = c(-3.5, 0),
  y = c(0.35, -0.07),
  labels = c("F(a)", "a"),
  col = "red"
)

xsample <- rnorm(20)
plot(
  ecdf(xsample),
  main = "pdf", xlab = "x", ylab = "F(x)",
  ylim = c(-0.1, 1), xlim = c(-4, 4)
)
segments(
  x0 = c(xstar, xstar),
  y0 = c(-1, ecdf(xsample)(xstar)),
  x = c(xstar, -5),
  y = c(ecdf(xsample)(xstar), ecdf(xsample)(xstar)),
  lty = 2, col = "red"
)
text(
  x = c(-3.5, 0),
  y = c(ecdf(xsample)(xstar) + 0.05, -0.07),
  labels = c("F(a)", "a"),
  col = "red"
)
```

---
# Egalité en distribution

Deux variables aléatoires réelles X et Y sont égale en distribution si et seulement si elles ont la même fonction de répartition :
    
$$X \stackrel{d}{=} Y \Leftrightarrow \operatorname{P}(X \le x) = \operatorname{P}(Y \le x)\quad\text{for all }x$$
Pour comparer deux échantillons, on peut se baser sur les diagnostiques graphiques suivants :

```{r qqplot, echo=FALSE, fig.height=3, dev='svg'}
par(mfrow = c(1, 3))


xsample <- rnorm(20)
xsample2 <- rnorm(20, mean = 0.05, sd = 0.99)
xylim <- range(xsample, xsample2) + c(-0.3, 0.3)

plot(
  ecdf(xsample),
  main = "pdf", xlab = "x", ylab = "F(x)",
  xlim = xylim
)
lines(
  ecdf(xsample2), col = "brown"
)
legend(
  "bottomright", legend = c(expression('X'[1]), ylab = expression('X'[2])),
  pch = 20, col = c("black", "brown")
)
qqplot(
  xsample, xsample2,
  xlab = expression('X'[1]), ylab = expression('X'[2]),
  xlim = xylim, ylim = xylim,
  pch = 20, main = "qq-plot", asp = 1
)
abline(a = 0, b = 1, col = "red")
xmerged <- sort(c(xsample, xsample2))
xmerged <- xmerged[seq(1, length(xmerged), 1)]
# xmerged <- seq(min(xmerged), max(xmerged), length.out = 20)

plot(
  ecdf(xsample)(xmerged),
  ecdf(xsample2)(xmerged),
  xlab = expression('F'[X[1]]*'(x)'), ylab = expression('F'[X[2]]*'(x)'),
  xlim = c(0, 1), ylim = c(0, 1),
  pch = 20, main = "pp-plot", asp = 1
)
abline(a = 0, b = 1, col = "red")
```

---
# Convergence en loi et  fonction de répartition

Considérons:

- une suite $X_1$, $X_2$,  ..., $X_n$, de variables aléatoires réelles ayant respectivement pour fonction de répartition  $F_1$, $F_2$, ..., $F_n$ 

- et une autre variable aléatoire réelle $X$ de fonction de répartition $F$

On dit que $(X_n)_{n\ge 0}$ converge en loi vers $X$ si et seulement si 

$$\lim_{n \rightarrow \infty}\ F_n(x) = F(x)$$

dès que  la fonction de répartition $F$ de $X$ est continue en $x$, ou bien, de manière équivalente, dès que $\mathbb P(X=x)=0$ 

**Remark**.  Quand on utilise par exemple  la convergence en loi, on parle de théorie asymptotique.

---
# Théorème Central Limite.

Si $X_1,X_2,\ldots,X_n$ sont des variables aléatoires réelles indépendantes de même loi de probabilité, d'espérance $\mu$ et de variance $\sigma^2$ alors, lorsque $n$ est suffisamment grand :

la variable aléatoire :
$$ S_n = X_1+X_2+\ldots+X_n$$
suit approximativement une loi normale d'espérance $\mu \times n$ et d'écart-type $\sigma\sqrt n$, notée :
$$\mathcal N(\mu n,\sigma\sqrt n)$$

source : [wikiversity](https://fr.wikiversity.org/wiki/Th%C3%A9or%C3%A8me_central_limite)



---
# Théorème de la valeur extrême.

Soit $X_1,X_2\ldots, X_n\ldots$ une séquence de variables indépendantes et identiquement distribuées et $M_n=\max\{X_1,\ldots,X_n\}$. Si une séquence de paires de nombres réels $(a_n, b_n)$ existe telle que $a_n>0$ et
$$\lim_{n \to \infty}P\left(\frac{M_n-b_n}{a_n}\leq x\right) = F(x)$$,
où $F$ est une fonction de distribution non dégénérée, alors la distribution limite $F$ appartient à la famille des lois de Gumbel, lois de Fréchet, ou des lois de Weibull. Ces familles peuvent être regroupées dans la classe des lois d'extremum généralisées de fonction de répartition:

$$F(x;\mu,\sigma,\xi) = \exp\left\{-\left[1+\xi\left(\frac{x-\mu}{\sigma}\right)\right]_+^{-1/\xi}\right\}$$

avec
$\left(1+\xi(x-\mu)/\sigma \right)_+=\max\left( 0 , 1+\xi(x-\mu)/\sigma \right)$
où $\mu\in\mathbb{R}$ est un paramètre de position, $\sigma > 0$ un paramètre de dispersion et $\xi\in\mathbb{R}$ un paramètre de forme appelé ''indice des valeurs extrêmes''.


source : [wikipedia](https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_de_Fisher-Tippett-Gnedenko)

---
# Loi d' extremum généralisée

Generalized Extreme Value distribution (GEV)
					
$$M_n = max\{X_1 , ... , X_n\},$$
où $X_l, ... , X_n$, est une séquence de variables aléatoires indépendantes et de fonction de répartition $F$.

**Theorem**
					
Si il existe  une séquence de constantes $\{a_n > 0\}$ et  $\{b_n\}$ tel que
$$\mathbb{P}\{(M_n - b_n)/a_n \leq z \} \rightarrow G(z) \text{ as } n \rightarrow \infty $$  

où  $G$ une fonction de répartition non-dégénérée, alors $G$ appartient à famille des distributions GEV

$$G(z) = \exp\left\{-\left[1 + \xi \left(\frac{z - \mu}{\sigma}\right)\right]^{-1/\xi}\right\},$$

définie sur $\{z : 1 + \xi (z - \mu) / \sigma \geq 0\}$, avec $-\infty < \mu < \infty$, $\sigma > 0$ et 	$-\infty < \xi < \infty$

---
# Loi d' extremum généralisée

**En pratique**, on ne cherche pas les constantes de normalisation  $\{a_n > 0\}$ et  $\{b_n\}$. On regarde simplement si les données sont cohérents avec une distribution GEV.


On utilise le fait que sous les hypothèse du théorème
$$\mathbb{P}\{M_n \leq z \} \approx G\{ (z - b_n)/a_n \} = G^*(z),$$
où $G^*$ est un autre membre de la familee des GEV.

---
# Block maxima
```{r blockmax, echo=FALSE, fig.height=3, dev='svg'}
  x = rnorm(100)
  block = rep(1:10, rep(10, 10))
  imax = tapply(x, block, function(x) x == max(x)) |> unlist()
  thresh = quantile(x, p = 0.9)
  iabove = x >= thresh
  col = c("grey", "red")
  par(mar = c(2, 3, 0, 1))
  plot(x, type = "n", xlab = "", ylab = "", cex.axis  = 2)
  points(x, col = col[imax+1], cex = 3, pch = 20)
  abline(v = 0.5 + 1:10 * 10, lty = 4, lwd = 3)
```

---
# Loi de pareto genéralisée

Generalized Pareto Distribution (GPD)

$$(X - u) | (X > u) $$

where X has a distribution function $F$

**Theorem**
Si lorsque  $n \rightarrow \infty$, $M_n = max\{X_1 , ... , X_n\}$ converge en distribution vers $G$ appartenant à la famille GEV, alors pour $u$ assez grand,  la fonction de répartition de  $(X - u) | (X > u) $  peut être approximée par 

$$H(y) = 1 - \left(1 + \frac{\xi y}{\tilde{\sigma}} \right)^{-1/\xi}$$

définie sur $\{y : y > 0$ avec $(\xi y / \tilde{\sigma}) > 0 \}$




---
# Loi de pareto genéralisée

 **Remark**.  On a une correspondance avec les paramétrés da la distribution GEV
 
 - $\tilde{\sigma} =  \sigma + \xi (u - \mu)$  
 
 - le paramètre de forme est le même que celui de la GEV associée.
 
 - le paramètre de forme  $\xi$, aussi appelé 'indice des valeurs extrêmes' charcterise le comportement de la queue de distribution: 
    - $\xi < 0$, la  distribution a une borne supérieure égale à $\mu - \tilde{\sigma} / \xi$ 
    - $\xi = 0$, la distribution a une queue à décroissance exponentielle. On interprète ce cas comme la limite de $\lim_{\xi \rightarrow 0} H(y)$
    - $\xi > 0$, la distribution est à queue lourde. 

---
# Peaks over threshold
```{r pot, echo=FALSE, fig.height=3, dev='svg'}
  par(mar = c(2, 3, 0, 1))
  plot(x, type = "n", xlab = "", ylab = "", cex.axis= 2)
  abline(h = thresh, lty = 4, lwd = 3)
  for(i in seq_along(x)){
    if(iabove[i]) segments(i, x[i],  i, thresh, lwd = 4)
  }
  points(x, col = col[iabove+1], cex = 3, pch = 20)
```
